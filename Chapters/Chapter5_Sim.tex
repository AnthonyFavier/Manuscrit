\newpage
\thispagestyle{empty}
\mbox{}

\ifdefined\included
\else
\setcounter{chapter}{4} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{Implementing a Joint Action Execution Scheme in an Interactive Simulator}
\chaptermark{Interactive Simulator}
\label{chap:5}
\minitoc

\chapabstract{
The joint action model we proposed in the previous chapter was abstracted and used to guide our planning approach. This chapter proposes implementing this model as an execution scheme integrated into a dedicated simulator. The latter allows human operators to collaborate with a simulated robot, executing the policies produced by our planning approach. We provide more details about our joint action model and technical details about the developed simulator used to conduct a user study, which will be described in the next chapter.
}

\section{Introduction}

In order to validate the approach presented in the previous chapter (\ref{chap:4}), we would like to be able to perform a task collaboratively with a simulated robot executing the produced policy.
For that, we refined our model of concurrent and compliant joint action execution. In Chapter~\ref{chap:4}, this model was abstracted and used to guide the planning process to explore further relevant courses of action. As another contribution, we implemented the model as an execution scheme capturing subtleties and social cues of a smooth collaboration. Then, we integrated this scheme into a simulator, allowing human operators to collaborate with a simulated robot running policies produced by our planning approach. An overview of the complete system is shown in figure~\ref{fig:architecture}.
Eventually, we used this simulator to conduct a user study, which will be described in the next chapter. 


\begin{figure}[h]
    \center
    \includegraphics[width=\textwidth]{Chapter5/architecture.pdf}
    \caption{Overview of the Interactive Simulator's Architecture}
    \label{fig:architecture}
\end{figure}

This chapter first presents details about the refined joint action model and highlights the additions compared with the abstracted model presented in Chapter~\ref{chap:4}. Then, technical details about its implementation and the overall simulator are given. After describing the simulated scene, details about the various controllers used to run the simulator are provided. We also explain how a human operator can interact with the simulator and effectively collaborate with the robot. Finally, we describe the data we collect during and after each collaboration. 


\section{Execution Controller - Joint Action Model for Execution}

The model of execution presented in the last chapter was simplified and abstracted to be used to guide the planning algorithms and explore further relevant courses of action. This allows for anticipating possible coordination and compliance with online human decisions. 
However, we need an execution controller to supervise the execution of the produced policy. Here, we propose a refined and implemented model, matching the abstraction previously made, to be able to execute the produced robot policy and supervise its execution by synchronizing with the human through social signals.

The next page depicts the complete model in fig~\ref{fig:complete_model_exec}. In addition to refining the `robot automaton' present in the abstracted version, it also models the human agent's behavior with another automaton that captures the possible decisions the human may make during execution. Additionally, this complete model makes all the social signals exchanged between the agents explicit and shows how they are used for coordination. These differences are discussed in detail below. This execution controller takes as input the solution \acrshort{dag} produced by the approach of Chapter~\ref{chap:4}. Then, the controller progresses from the initial state to a goal state in the solution graph. The controller handles the state transitions, step by step, by sending and synchronizing on social cues. Thanks to this controller and the associated simulator, we now have a durative task execution that effectively affects a simulated environment.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\linewidth]{Chapter5/complete_automaton.pdf}
    \caption{Complete Model of Concurrent and Compliant Joint Action used as an execution scheme. This version details the robot automaton and the assumed human automaton as well as the visual signals exchanges between the agents to synchronize themselves.}
    \label{fig:complete_model_exec}
\end{sidewaysfigure}


In the execution controller, the robot automaton is refined and more detailed. 
First, we show the social cues sent by the robot, which are the following: \textit{START} corresponds to the visual and audible signal indicating to the human that the step begins; \textit{S\_RA} corresponds to the start of a robot action; \textit{R\_PASS} corresponds to the robot being explicitly passive. The other signals, including the human ones, will be described below.
Also, we modeled the case where the robot cannot act in the current step. This can be determined easily given the search graph and the current state $s_i$ and by checking if there is at least one non-passive robot action in the following arcs. This checking process is represented in the figure by the diamond shape with the label ``R can act'', and the respective positive and negative answers ``yes'' (y) and ``no'' (n). When the robot cannot act, it sends a specific signal to inform the human that the step has started and cannot act (START\_IDLE), then goes in the \textit{IDLE} pose. This is a visually passive posture where the robot looks at the human with its arm retracted, indicating that it cannot act. Only a human action can progress to the next state. Thus, the robot waits for this action to start and eventually finish. Then, the Situation Assessment process identifies the executed action, and thus, the new state $s_{i+1}$ to continue. Depending on the possible actions, the robot can stay in this \textit{IDLE} pose for several steps. Once the robot can act again, the ``yes'' branch of the automaton is followed, and the robot returns to the \textit{HOME} pose. In this posture, the robot's arm is deployed, showing that the robot is ready to grab objects on the table.

Additionally, the process of waiting for a human signal at the beginning of each step is more detailed. This process is interrupted by one of the three following signals: \textit{H\_PASS} is an explicit signal (hand gesture, verbal communication) sent by the human to share its desire to be passive; \textit{S\_HA} is the start of human actions, and is detected by tracking human's motion; \textit{TO} is the internal timeout signal sent by a clock $4s$ after the start of the waiting process. 
However, the perception layer on a real robot would induce a delay in this synchronization process. 
An additional waiting time of $1s$ is added to compensate for the robot's reaction time, allowing the human to send signals just before the timeout. Without it, the human could start moving just before the timeout, but the robot would interpret this information too late and consider them as passive.
Note that the robot only waits for a human signal if the human can act in the current step. This check is done similarly to the first diamond process. However, it is not shown in the figure for legibility reasons. Hence, it means that in situations where only the robot can act, it will not wait and will directly start acting.

% Remember that this process only identifies an explicit signal like a predefined hand gesture or the start of a human motion. In the case of human motion, we do not yet identify which action the human is starting. This is achieved by the dedicated Identification (ID) process, which is longer and here about $0.6s$. Thus, we believe that the chosen duration of $0.3s$ is relevant for detecting the initial human signals. 

The last visual difference with the abstracted model on the robot's side is the case where the human decides to be passive. In this branch, the human decision is pictured as a question mark. This is because we first identified the human as passive, and thus, the robot performs the best corresponding action provided by the policy. However, while the robot is acting, the human is free to start performing any action that does not conflict with the already-started robot's action. These possible decisions are modeled in the human automaton described after but are identified by the robot only during the Situation Assessment process. 

On the other hand, a human automaton now explicitly models how humans should behave to coordinate with robots and collaborate successfully. This automaton starts by always waiting for the beginning of the current step, indicated by the robot with the signals \textit{START} or \textit{START\_IDLE}. The simplest but less common case is when the robot cannot act during the step (after receiving \textit{START\_IDLE}). This case is depicted in the rightmost branch. Since the robot cannot act, only the human can make progress in the task by acting. Thus, the human is free to take as much time as desired, represented by the decision time in the range $[0; +\infty]$. After the start of the human action, both human and robot automatons wait for the end of the human action, and then they proceed.
Let us discuss now the most common case where the robot can act. This time, the human must decide before the defined robot timeout (TO), otherwise they will be considered passive. The human can decide either to start acting (ACT), to indicate their passivity (PASS), or do nothing corresponding to being passive without informing the robot (WAIT). If the human decides to be passive (PASS or WAIT cases), they can either remain passive until the next step or decide to perform an action not conflicting with the robot one. This is represented by the last human diamond shape labeled ``Compliant choice'' made after a duration referred to as ``compliant decision time''.


\section{Gazebo Simulated Scene}

This interactive simulator is based on the Gazebo Simulator. It is an open-source 3D simulator able to simulate articulated robots in a dynamic and complex environment.
This section describes all aspects directly linked to this existing 3D simulator on which our framework has been developed.

The overall scene is depicted in the figure~\ref{fig:simu_view}. The stacking goal is displayed in the top left corner, and a text prompt is shown in the top right corner so that the robot can communicate with the human.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Chapter5/simu_screenshot.png}
    \caption{Participant view of the interactive simulator. It depicts the colored cubes on the table's left, the stacking area on the table's right (the two darker spots), the goal pattern on the top left, the collaborative robot in front, the text prompt on the top right, and the human hand at the bottom right.}
    \label{fig:simu_view}
\end{figure}

The static scene consists of a large room, with a ground and four walls, and a table placed in the center with two marks on its right side. The marks represent the locations where the cubes must be stacked. The agents are represented as follows. First, the human is represented only with a floating hand as the camera simulates a first-person point-of-view. Then we used a Tiago robot from PAL Robotics because it has an arm to manipulate its environment, a head to send gaze information and signals, and because many related resources are available for it (3D models, controllers, tutorials). The human and the robot are facing each other, each from one opposite side of the table. Finally, colored cubes were disposed of on the table in three distinct zones. Cubes are either close to one agent's side or in the middle of the table. This disposition influences the reachability of the agents as one agent can only reach cubes from their side (just in front of them) or the middle, but never from the other agent's side. This scene corresponds to one specific collaborative task, but another setup could be easily implemented. 

The Gazebo simulator can be integrated with the \acrfull{ros}, which is a set of software libraries and tools that help build robot applications. The main pros of \acrshort{ros} are its ability to run several sub-programs in parallel and allow them to communicate with each other. Hence, the other components of the interactive simulator have been developed with \acrshort{ros}.


\section{Controllers}

Four distinct controllers have been developed to control the simulated agents and the overall simulation execution. Each controller is described below. 

\subsection{Robot Arm Motion}

First, there is one controller that moves the robot's arm. This controller is called with either a 3D position to reach (pose target) or a predefined configuration (named target). In both cases, when called, this controller uses the MoveIt framework. MoveIt creates links between several libraries in order to have access to a unified interface for Motion Planning algorithms, Inverse Kinematics, Control, or Collision Checking. This way, our controller can ``simply'' request MoveIt to find a trajectory to a given position and then make the robot follow this trajectory while taking into account the geometry of the robot arm.

In fact, this controller is slightly more sophisticated. Indeed, in our collaborative context, we prefer the robot to be reactive rather than to have optimal motions. This is why, through the MoveIt interface, we use two different motion planners. The first one is $RRT^*$, which is an optimal planner that stops only when the optimal motion plan has been found. It is desirable to have the robot exhibit consistent and efficient motions. However, this process sometimes takes too much time (4-5s), which breaks the rhythm of the interaction. As a consequence, a short timeout (0.6s) has been set for this optimal motion planner. If the optimal plan is not found within the defined timeout, we use the second motion planner. 
The second planner is $SBL$, which is not an optimal planner. That is, the planner stops when a solution is found, but there is no guarantee of optionality. The major pro of this planner is its speed.
Overall, for every robot arm motion, we first try to find an optimal motion within a short amount of time. If the optimal motion is not found, we quickly find a suboptimal solution to prevent the robot from being passive for too long.

Additionally, to reduce motion planning time, a few simplifications have been done. 
First, collisions are only considered with the table and the robot's body. Hence, the robot arm sometimes goes through the other cubes.
Also, the pick and place orientation are ignored. When picking a cube, the robot gripper just reaches the cube's center from any angle, and then the cube is attached to the gripper. When placing a cube, the robot moves its arm to the target position, and the cube is detached from the gripper. Eventually, the cube's orientation and position are overwritten to match the target location. This way, the robot always performs perfect place action, improving both the reactivity of the robot and its robustness, but the robot's motions are less realistic.

\subsection{Robot Head Motion}

The robot head is controlled to look at various elements during the interaction to exhibit a more intuitive and collaborative behavior. 
Indeed, when waiting for the human, the robot looks at the camera. As soon as the human hand moves to either perform an action or indicate its passivity, the robot starts following the hand to show it is observing the human motions and estimating their intentions.
Then, the robot looks at a cube to indicate its intention to pick it since this is faster than the arm motion. 
Also, the robot looks at the target position when placing a cube to indicate its intention to place the cube there.

A dedicated controller has been developed to perform those head motions based on an existing controller provided in the Tiago robot modules. The existing controller could only change the robot's gaze through a visual and clickable window, allowing an operator to manually click on the scene to change the robot's gaze. This controller has been modified to allow additional features, such as directly requesting to look at a 3D point or an object. Moreover, we can now ask to follow an object. That's how we are able to exhibit the head behavior described just above.  

\subsection{Human Hand Motion}

Moving the human hand requires a dedicated controller but is much simpler than the robot arm motion one. Here, the hand must either move to a position or perform a PASS signal. The PASS signal is a hand gesture indicating to the robot that the human desires to be passive for the current step. This motion is performed by simply rotating the hand back and forth at a constant speed. The PASS motion has a duration of $0.7s$. 
On the other hand, we do not use a real motion planner to move the hand to a target position. We simply compute a straight line between the current hand pose and the target pose. Then, we update the hand position at $50Hz$ to move at a constant speed set to $0.25 m/s$. The planning time is thus negligible compared to the robot motion planning times.

\subsection{Simulation}

Last but not least, I developed a fourth controller called the \textit{simulation controller}. Its main jobs are to decompose high-level actions into low-level commands for the simulator, to keep track of the step-based synchronization used in our joint action model, and to generate the visual social signals used in our model to coordinate the agents. 

\subsubsection{Action Decomposition}

First, we describe how we translate the high-level actions from the plan produced by our task planner to low-level motions that can be executed by the controllers listed above. Some parts of this controller are domain-specific, but a major part is generic for manipulation tasks. This controller received high-level agent actions to execute, such as ``Robot pick(b1)'' for the robot picking the cube b1. The first step is to decompose this action into low-level generic actions, which are themselves made of low-level motions. This controller also keeps track of a few low-level facts to check low-level preconditions, such as what each agent is holding or which object is still on the table. Hence, the controller throws an error when an agent tries to pick a cube while already holding one.

Let's first comment on the low-level generic actions available. This controller is given a list of the object names (cubes) and the names of some predefined locations in the scene (placing locations in the stack). Except for the last five, the following actions are not agent-specific and can be performed by any agent. This is defined by a parameter given when calling the action. The following low-level actions are available:

\begin{itemize}
    \itemsep0em
    \item \textbf{move\_pose\_target:}          moves either the hand/robot arm to a given position.
    \item \textbf{move\_location\_target:}      moves either the hand/robot arm to the position of a given location.
    \item \textbf{move\_obj\_target:}           moves the hand/robot arm to the position of a given object.
    \item \textbf{move\_home:}                  puts the agent in its ``home'' (default) configuration. 
    \item \textbf{move\_named\_target:}         moves the robot arm to the given configuration.
    \item \textbf{grab\_obj:}                   attaches the given object to the hand/robot gripper. 
    \item \textbf{drop\_obj:}                   detaches the given object to the hand/robot gripper.
    \item \textbf{set\_obj\_rpy:}               sets the orientation of a given object.
    \item \textbf{set\_obj\_pose:}              sets the position of a given object.
    \item \textbf{delta\_move\_obj:}            sets the position of a given relative to its current position.
    \item \textbf{human\_hand\_gesture:}        makes a hand gesture to indicate passivity.
    \item \textbf{robot\_head\_look\_pose:}     makes the robot look at a given position.
    \item \textbf{robot\_head\_look\_human:}    makes the robot look at the human (camera).
    \item \textbf{robot\_head\_follow\_obj:}    makes the robot follow a given object.
    \item \textbf{robot\_head\_follow\_hand:}   makes the robot follow the human hand.
\end{itemize}

Then, the high-level actions from the plan produced are the following with their respective simplified low-level decomposition:

\begin{itemize}
    \item \textbf{PickCube:}            Starts by retrieving the cube's current position by sending a request to the Gazebo simulator. If it is the robot, call robot\_head\_look\_obj to look at the cube. Then, call move\_pose\_target with the retrieved cube position. Once over, grab the cube with grab\_obj. After, move\_home is called to retract the robot arm or bring the hand to its initial position. Finally, if it is the robot, it looks to the human with robot\_head\_look\_human.
    
    \item \textbf{PlaceCube:}           Starts by retrieving the position of the target location to stack the cube. If it is the robot, call robot\_head\_look\_pose with this position. Then, we move either the arm or the hand to that position with move\_pose\_target before dropping the cube in the stack with drop\_obj and adjusting its position. After, we go back to the home configuration with move\_home. And for the robot, we look at the human with robot\_head\_look\_human. 
    
    \item \textbf{BePassive:}           The human waves their hand to explicitly be passive. Thus, human\_hand\_gesture is called. The robot does not move and only displays some texts saying the robot wants to be passive. More details about the text prompts will be provided later.
    
    \item \textbf{DropCubeTable:}       When an agent cannot stack a cube they are holding, they can place it back on the table. First, the drop position is defined. It corresponds to the initial position of the cube being held, except for the green one, which is initially on top of another cube and has a dedicated drop position. The robot looks at the drop position with robot\_head\_follow\_pose. The hand or the robot arm is moved to the position with move\_pose\_target. The cube is dropped with drop\_obj, and its position is adjusted. Then, the agent is put in its home configuration with move\_home, and the robot looks at the human with robot\_head\_look\_human.  
\end{itemize}

\subsubsection{Manage Steps Synchronizations}

The simulator controller is also in charge of indicating when a step is over. Since we do not consider steps where both agents are passive, a step begins when an agent starts an action. A step is over when both agent actions are done. These rules cover many situations, such as if the human initially wants to be passive and wave their hand. As a result, the robot starts performing an action, which starts the next step. Currently, the step would be over as soon as the robot's action is over since the human is passive. However, if the human decides to start performing an action concurrently, then the step will be over when both the robot's and human's actions are over. This may imply that if the human starts acting right before the end of the robot's action, then the robot will just wait for the end of the human action, and thus, the end of the step before the next step begins.  

\subsubsection{Send Visual Signals}

The model of execution synchronizes the agents based on explicit visual signals such as the start of a step, the start of an action, the end of an action, or a hand gesture (PASS). Those signals are modeled explicitly inside the system and managed by the simulator controller.

Indeed, when starting an action, the associated visual signal is sent only when the agent starts to move. Hence, we track the arm and hand motions to know when the action is visible to the other agent. Then, when an action is over, the associated visual signal is sent directly. It is worth mentioning that humans will naturally have a reaction time when seeing the robot's visual signals (start/end of action, step start). However, the robot natively does not have any reaction time to such symbolic visual signals. Thus, to simulate the delay introduced by an actual perception module (here perfectly simulated), the human visual signals are delayed to the robot by a reaction time set to 0.3 seconds. This is still quite fast, but at least the robot does not interpret human motions instantly.   




\section{Human-Machine Interface (HMI)}

The human operator, or participant, can interact with the simulation by performing any feasible action during the process. However, humans must synchronize with robots by following the execution model. For this purpose, every step starts with a robot sound/beep. Additionally, a dedicated process receives internally the list of the feasible human actions for the current step. This list is sent by the robot execution scheme and is extracted from the solution \acrshort{dag}. However, the participants do not see or know of feasible actions.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Chapter5/simu_screenshot_HMI.png}
    \caption{Hidden clickable areas triggering human actions, if feasible. Areas from 1 to 5 trigger picking the respective cube if it is on the table and the human is not holding one already. Area 6 corresponds to placing a held cube in the stack. Area 7 makes a hand gesture indicating the human desire to be passive to the robot.}
    \label{fig:hmi}
\end{figure}

Each possible human action is associated with a rectangular clickable area on the screen. When the human clicks in an area associated with a currently feasible action, the process requests the simulation controller to perform the corresponding human action. Areas 1 to 5 request to pick the corresponding cube if it is on the table and the human is not holding one already. Area 6 corresponds to placing a cube being held into the stack. Since cubes only have one possible stacking place at a time for this task, we consider a large area to trigger placing actions. The controllers determine the exact location to place the cubes. Finally, area 7 makes a hand gesture indicating the human desire to be passive to the robot. It corresponds to sending the \textit{H\_PASS} signal shown in figure~\ref{fig:complete_model_exec}.

\section{Execution Data: Logs, Metrics, and Timeline}

There is also a dedicated process to record the different events and signals during one execution. All other components can send events to log to this process to save them for later post-processing. A log event is defined with a name and a time stamp. Additionally, visual signals are also captured by this logging process and saved both as a signal and as a corresponding event. Logged events mostly help trace the robot's internal states and processes during the execution. The human ones mainly serve to identify when the human is acting or not since we do not have access to the internal reasoning process of the human participant. 

\subsection{Extraction of Agents' Activities}

All logs are saved after each execution and can be loaded later for post-processing. The post-processing involves three steps. First, the agent activities are extracted from the event list. The human and the robot have different possible activities, but since we do not have access to internal human reasoning, there are more robot activities than human ones. The extracted activities are shown in the following table~\ref{tab:agent_activities}. The activities' orders try to match what can happen during the execution, but not all activities are present at every step. Indeed, on the robot side, the ID process is not always necessary and thus not always executed, and the robot either performs an action or is passive for diverse reasons. But the robot always ends up waiting for the end of the step, performing the \acrshort{sa} process and getting ready for the next step. On the human side, if the human performs an action or makes a hand gesture, there will be a measurable decision time. If the human remains passive without signaling the robot, then the only activity is passive without signaling. The ``waiting next step to start'' activity is only added if the human is active. Otherwise, being passive for a step is implicitly equivalent to waiting for the next step.  

\begin{table}[]
    \begin{tabular}{|l|l|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Robot Activities}}                                           & \multicolumn{1}{c|}{\textbf{Human Activities}}                                                       \\ \hline
    Waiting for human decision                                                                & Decision Time                                                                                        \\ \hline
    Identification Process (ID process)                                                       & \multirow{3}{*}{Human Action}                                                                        \\ \cline{1-1}
    Planning arm motion                                                                       &                                                                                                      \\ \cline{1-1}
    Robot Action                                                                              &                                                                                                      \\ \hline
    \begin{tabular}[c]{@{}l@{}}Waiting for human action when\\ itself cannot act\end{tabular} & \begin{tabular}[c]{@{}l@{}}Being passive with signaling \\ (after PASS hand gesture)\end{tabular}    \\ \hline
    Being passive                                                                             & \begin{tabular}[c]{@{}l@{}}Being passive without signaling\\ (after TimeOut is reached)\end{tabular} \\ \hline
    Waiting for the end of the step                                                           & \multirow{3}{*}{Waiting for the next step to start}                                                  \\ \cline{1-1}
    Situation Assessment Process (SA process)                                                 &                                                                                                      \\ \cline{1-1}
    Getting ready for the next step (GRNS)                                                    &                                                                                                      \\ \hline
    \end{tabular}
    \caption{Agent activities extracted from logs. The activities' orders and relative positions try to match the execution, but not all activities are present at every step.  }
    \label{tab:agent_activities}
    \end{table}

\subsection{Metrics Computation}

When loading the execution logs, we extract a set of metrics from the events and extracted activities. Currently, 27 metrics are extracted, but the following table \ref{tab:metrics} only shows 11 because several sub-metrics are hidden. Indeed, for four of the listed metrics, we compute the sum, average, standard deviation, min, and max values of the corresponding metric.

% {\small
% \begin{enumerate}
%     \itemsep0em
%     \item \textbf{Task completion time:}                 Time for the stack to be completed.
%     \item \textbf{Number of steps:}                      Number of steps executed.
%     \item \textbf{Number of human optimal actions:}      Number of human action being optimal regarding a given objective.
%     \item \textbf{Ratio of human optimal actions:}       Number of human optimal actions divided by the number of steps.
%     \item \textbf{Decision time:}                        Time for the human to make a visible decision in a step. Five different metrics are extracted regarding all steps of the scenario: total/sum, average, standard deviation, min and max.
%     \item \textbf{Waiting Next Step:}                    Time during which the human waits for the next step to start. Again, are extracted over all the steps: total, average, standard deviation, min and max values.
%     \item \textbf{Number of human action:}               Number of human actions executed in the scenario.
%     \item \textbf{Human action duration:}                Over all steps are extracted the total, average, standard deviation, min and max duration of the human actions.
%     \item \textbf{Number of robot action:}               Number of robot actions executed in the scenario.
%     \item \textbf{Robot action duration:}                Over all steps are extracted the total, average, standard deviation, min and max duration of the robot actions.
%     \item \textbf{Time human free:}                      Time after which the human is free, i.e., is not mandatory for the task to be completed.
% \end{enumerate}
% }

\begin{table}[]
    \begin{tabular}{|c|l|}
    \hline
    \textbf{Metric}                                                           & \multicolumn{1}{c|}{\textbf{Description}}                                                                                   \\ \hline
    Task Completion Time                                                      & Time for the task to be completed.                                                                                          \\ \hline
    Number of steps                                                           & Number of executed steps.                                                                                                   \\ \hline
    \begin{tabular}[c]{@{}c@{}}Number of optimal\\ human actions\end{tabular} & \begin{tabular}[c]{@{}l@{}}Number of human action being optimal regarding a \\ given objective.\end{tabular}                \\ \hline
    \begin{tabular}[c]{@{}c@{}}Ratio of optimal\\ human actions\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Number of optimal human action divided by the \\ number of steps.\end{tabular}                   \\ \hline
    Human decision time*                                                      & \begin{tabular}[c]{@{}l@{}}Duration for the human to make a visible decision \\ in a step.\end{tabular}                     \\ \hline
    Waiting next step*                                                        & \begin{tabular}[c]{@{}l@{}}Duration during which the human waits for the next \\ step to start.\end{tabular}                \\ \hline
    \begin{tabular}[c]{@{}c@{}}Number of \\ human action\end{tabular}         & \begin{tabular}[c]{@{}l@{}}Number of non-passive human actions executed in \\ the scenario.\end{tabular}                    \\ \hline
    \begin{tabular}[c]{@{}c@{}}Human action \\ duration*\end{tabular}         & Duration of the human actions.                                                                                              \\ \hline
    \begin{tabular}[c]{@{}c@{}}Number of \\ robot action\end{tabular}         & \begin{tabular}[c]{@{}l@{}}Number of non-passive robot actions executed in \\ the scenario.\end{tabular}                    \\ \hline
    \begin{tabular}[c]{@{}c@{}}Robot action \\ duration*\end{tabular}         & Duration of the robot actions.                                                                                              \\ \hline
    Time human free                                                           & \begin{tabular}[c]{@{}l@{}}Time after which the human is free, i.e., as soon as \\ the robot can finish alone.\end{tabular} \\ \hline
    \end{tabular}
    \caption{Metrics extracted from the execution logs for each scenario. Marked items with a star (*) correspond to five sub-metrics computed over all steps: sum, average, standard deviation, min, and max values.}
    \label{tab:metrics}
    \end{table}

\subsection{Execution Example with Detailed Timeline}

A complete scenario is presented and commented on in this section. Using the previously extracted agent activities, we can draw a visual timeline depicting the different steps, the extracted activities, and the visual signals exchanged between the agents. Figure \ref{fig:timeline_example} depicts the complete commented timeline, including agents' activities, exchanged signals, and snapshots from the simulator. The table \ref{tab:signals} describes the different signals exchanged during the execution and shown on the timeline. 

\begin{table}[h]
    \begin{tabular}{|c|l|}
    \hline
    \textbf{Signal name} & \multicolumn{1}{c|}{\textbf{Description}}                                                                                        \\ \hline
    START                & \begin{tabular}[c]{@{}l@{}}Visual and audible signal from the robot indicating that the\\ current step has started.\end{tabular} \\ \hline
    START\_IDLE          & \begin{tabular}[c]{@{}l@{}}Visual and audible signal indicating the start of a step where\\ the robot cannot act.\end{tabular}   \\ \hline
    S\_RA / E\_RA        & \begin{tabular}[c]{@{}l@{}}Signals sent respectively from the start (S) and the end (E) of \\ a robot action (RA).\end{tabular}  \\ \hline
    S\_HA / E\_HA        & \begin{tabular}[c]{@{}l@{}}Signals sent respectively from the start (S) and the end (E) of\\ a human action (HA).\end{tabular}   \\ \hline
    H\_PASS              & \begin{tabular}[c]{@{}l@{}}Explicit hand gesture from the human informing their desire\\ the be passive.\end{tabular}            \\ \hline
    TO                   & \begin{tabular}[c]{@{}l@{}}The TimeOut is an internal robot signal indicating that the \\ absence of human signal.\end{tabular}  \\ \hline
    \end{tabular}
    \caption{Signals exchanged between the agents during the execution and used for synchronization and coordination.}
    \label{tab:signals}
    \end{table}

\begin{figure}
    \centering
    \includegraphics[height=0.97\textheight, keepaspectratio]{Chapter5/last_timeline.png}
    \caption{Complete execution timeline commented.}
    \label{fig:timeline_example}
\end{figure}

In this execution example, according to the extracted metrics, the task has been completed in $77.14s$ in $12$ steps. The average human decision time is $1.00s \pm 0.69s$ with $max=2.77s$ and $min=0.42s$. The human waited on average for the start of the next step during $1.68s \pm 1.28s$. There were $8$ human actions with an average duration of $3.67s \pm 0.72s$. According to the objective/preferences to finish the task as fast as possible, the human acted $9$ times optimally out of $12$ steps (optimal ratio$=75\%$). Indeed, the human agent purposely decided to be passive during steps 5 and 6, but they could have picked the orange cube in parallel to complete the stack faster. In this example, the task description forbids agents from picking cubes in advance. They must be able to immediately place a cube in the stack to pick it. This rule is detailed and justified later in the next chapter \ref{sec:study_protocol}. Hence, in step 9, the human picked the white cube, but the robot could not pick the bar until the white cube was placed. Thus, the human could have let the robot place the white cube to reduce their effort without slowing the task.
There were $8$ robot actions with an average duration of $3.97s \pm 0.69s$. Overall, the robot took $5.25s$ to plan its arm motions. 


